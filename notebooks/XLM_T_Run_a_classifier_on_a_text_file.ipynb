{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "XLM-T - Run a classifier on a text file",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaoDr57T74ag"
      },
      "source": [
        "## Installs and imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXC3dguV3cDb"
      },
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install sentencepiece\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWLw1TOn6P_u"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from scipy.special import softmax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhakJgjlIvC0"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aP-pGTWHFH7"
      },
      "source": [
        "def preprocess(corpus):\n",
        "  outcorpus = []\n",
        "  for text in corpus:\n",
        "    new_text = []\n",
        "    for t in text.split(\" \"):\n",
        "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
        "        t = 'http' if t.startswith('http') else t\n",
        "        new_text.append(t)\n",
        "    new_text = \" \".join(new_text)\n",
        "    outcorpus.append(new_text)\n",
        "  return outcorpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqnM48t2MtLG"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/cardiffnlp/xlm-t/main/data/sentiment/all/test_text.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGWH45yrIkOH"
      },
      "source": [
        "dataset_path = './test_text.txt'\n",
        "dataset = open(dataset_path).read().split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTdC_qDhLO-I",
        "outputId": "e6be8f7c-0938-4b49-d892-cc209a6da99d"
      },
      "source": [
        "# this is a dataset in 8 different languages\n",
        "for example in [0,870,1740,2610,3480,4350,5220,6090]:\n",
        "  print(dataset[example])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "نوال الزغبي (الشاب خالد ليس عالمي) هههههههه أتفرجي على ها الفيديو يا مبتدئة http vía @user\n",
            "Trying to have a conversation with my dad about vegetarianism is the most pointless infuriating thing ever #caveman \n",
            "Royal: le président n'aime pas les pauvres? \"c'est n'importe quoi\" http …\n",
            "@user korrekt! Verstehe sowas nicht...\n",
            "CONGRESS na ye party kabhi bani hoti na india ka partition hota nd na hi humari country itni khokhli hoti   @ \n",
            "@user @user Ma Ferrero? il compagno Ferrero? ma il suo partito esiste ancora? allora stiamo proprio frecati !!!\n",
            "todos os meus favoritos na prova de eliminação #MasterChefBR\n",
            "@user jajajaja dale, hacete la boluda vos jajaja igual a vos nunca se te puede tomar en serio te mando un abrazo desde Perú!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1S6wUeuqIsVI"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8v8HPwj6P_y"
      },
      "source": [
        "CUDA = True # set to true if using GPU (Runtime -> Change runtime Type -> GPU)\n",
        "BATCH_SIZE = 32\n",
        "MODEL = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)\n",
        "config = AutoConfig.from_pretrained(MODEL) # used for id to label name\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "if CUDA:\n",
        "  model = model.to('cuda')\n",
        "_ = model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9fsx8PPInt-"
      },
      "source": [
        "## Forward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-Bhdf7cGsIX"
      },
      "source": [
        "def forward(text, cuda=True):\n",
        "  text = preprocess(text)\n",
        "  encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "  if cuda:\n",
        "    encoded_input.to('cuda')\n",
        "    output = model(**encoded_input)\n",
        "    scores = output[0].detach().cpu().numpy()\n",
        "  else:\n",
        "    output = model(**encoded_input)\n",
        "    scores = output[0].detach().numpy()\n",
        "  \n",
        "  scores = softmax(scores, axis=-1)\n",
        "  return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRCMvwm7GsK3"
      },
      "source": [
        "dl = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "all_preds = []\n",
        "for idx,batch in enumerate(dl):\n",
        "  print('Batch ',idx+1,' of ',len(dl))\n",
        "  text = preprocess(batch)\n",
        "  scores = forward(text, cuda=CUDA)\n",
        "  preds = np.argmax(scores, axis=-1)\n",
        "  all_preds.extend(preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYzTpKcrGsNH",
        "outputId": "401f25b6-92cb-45bf-e8bb-ceaf213b02eb"
      },
      "source": [
        "# this is a dataset in 8 different languages\n",
        "for example in [0,870,1740,2610,3480,4350,5220,6090]:\n",
        "  pred = all_preds[example]\n",
        "  print(dataset[example], '--->', config.id2label[pred])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "نوال الزغبي (الشاب خالد ليس عالمي) هههههههه أتفرجي على ها الفيديو يا مبتدئة http vía @user ---> Neutral\n",
            "Trying to have a conversation with my dad about vegetarianism is the most pointless infuriating thing ever #caveman  ---> Negative\n",
            "Royal: le président n'aime pas les pauvres? \"c'est n'importe quoi\" http … ---> Negative\n",
            "@user korrekt! Verstehe sowas nicht... ---> Negative\n",
            "CONGRESS na ye party kabhi bani hoti na india ka partition hota nd na hi humari country itni khokhli hoti   @  ---> Negative\n",
            "@user @user Ma Ferrero? il compagno Ferrero? ma il suo partito esiste ancora? allora stiamo proprio frecati !!! ---> Negative\n",
            "todos os meus favoritos na prova de eliminação #MasterChefBR ---> Positive\n",
            "@user jajajaja dale, hacete la boluda vos jajaja igual a vos nunca se te puede tomar en serio te mando un abrazo desde Perú! ---> Neutral\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}